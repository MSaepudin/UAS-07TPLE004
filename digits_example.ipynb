{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0., 10., 14., 11.,  3.,  0.,  0.,  0.,  4., 16., 13.,  6.,\n",
       "       14.,  1.,  0.,  0.,  4., 16.,  2.,  0., 11.,  7.,  0.,  0.,  8.,\n",
       "       16.,  0.,  0., 10.,  5.,  0.,  0.,  8., 16.,  0.,  0., 14.,  4.,\n",
       "        0.,  0.,  8., 16.,  0.,  1., 16.,  1.,  0.,  0.,  4., 16.,  1.,\n",
       "       11., 15.,  0.,  0.,  0.,  0., 11., 16., 12.,  3.,  0.,  0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1167bc70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.gray() \n",
    "plt.matshow(digits.images[30]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target\n",
    "# Membagi data training dan testing(80:20)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size= 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1437, 64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1437,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(50,50,50),  max_iter=1000, alpha=0.0001,activation='logistic',\n",
    "                     solver='adam', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31652683\n",
      "Iteration 2, loss = 2.29852381\n",
      "Iteration 3, loss = 2.29087698\n",
      "Iteration 4, loss = 2.28419247\n",
      "Iteration 5, loss = 2.27628656\n",
      "Iteration 6, loss = 2.26563207\n",
      "Iteration 7, loss = 2.25229961\n",
      "Iteration 8, loss = 2.23566462\n",
      "Iteration 9, loss = 2.21463834\n",
      "Iteration 10, loss = 2.18647735\n",
      "Iteration 11, loss = 2.15305599\n",
      "Iteration 12, loss = 2.11323363\n",
      "Iteration 13, loss = 2.06422906\n",
      "Iteration 14, loss = 2.00763113\n",
      "Iteration 15, loss = 1.94520857\n",
      "Iteration 16, loss = 1.87683013\n",
      "Iteration 17, loss = 1.80304863\n",
      "Iteration 18, loss = 1.72762864\n",
      "Iteration 19, loss = 1.64998709\n",
      "Iteration 20, loss = 1.57439294\n",
      "Iteration 21, loss = 1.50114443\n",
      "Iteration 22, loss = 1.43287961\n",
      "Iteration 23, loss = 1.36617428\n",
      "Iteration 24, loss = 1.30515034\n",
      "Iteration 25, loss = 1.24696779\n",
      "Iteration 26, loss = 1.19281163\n",
      "Iteration 27, loss = 1.14239040\n",
      "Iteration 28, loss = 1.09547411\n",
      "Iteration 29, loss = 1.05077538\n",
      "Iteration 30, loss = 1.00745524\n",
      "Iteration 31, loss = 0.96865474\n",
      "Iteration 32, loss = 0.92992957\n",
      "Iteration 33, loss = 0.89415897\n",
      "Iteration 34, loss = 0.85946165\n",
      "Iteration 35, loss = 0.82452798\n",
      "Iteration 36, loss = 0.79371801\n",
      "Iteration 37, loss = 0.76497834\n",
      "Iteration 38, loss = 0.73578401\n",
      "Iteration 39, loss = 0.70956755\n",
      "Iteration 40, loss = 0.68405454\n",
      "Iteration 41, loss = 0.65925827\n",
      "Iteration 42, loss = 0.63713386\n",
      "Iteration 43, loss = 0.61623677\n",
      "Iteration 44, loss = 0.59507975\n",
      "Iteration 45, loss = 0.57370769\n",
      "Iteration 46, loss = 0.55462419\n",
      "Iteration 47, loss = 0.53641967\n",
      "Iteration 48, loss = 0.51865872\n",
      "Iteration 49, loss = 0.50082539\n",
      "Iteration 50, loss = 0.48413837\n",
      "Iteration 51, loss = 0.46841456\n",
      "Iteration 52, loss = 0.45310632\n",
      "Iteration 53, loss = 0.43829380\n",
      "Iteration 54, loss = 0.42444073\n",
      "Iteration 55, loss = 0.41075150\n",
      "Iteration 56, loss = 0.39727208\n",
      "Iteration 57, loss = 0.38580119\n",
      "Iteration 58, loss = 0.37375379\n",
      "Iteration 59, loss = 0.36170553\n",
      "Iteration 60, loss = 0.35115553\n",
      "Iteration 61, loss = 0.34057522\n",
      "Iteration 62, loss = 0.32948669\n",
      "Iteration 63, loss = 0.31827186\n",
      "Iteration 64, loss = 0.30898605\n",
      "Iteration 65, loss = 0.29811596\n",
      "Iteration 66, loss = 0.28881568\n",
      "Iteration 67, loss = 0.27909852\n",
      "Iteration 68, loss = 0.27026466\n",
      "Iteration 69, loss = 0.25927098\n",
      "Iteration 70, loss = 0.25165892\n",
      "Iteration 71, loss = 0.24322046\n",
      "Iteration 72, loss = 0.23401581\n",
      "Iteration 73, loss = 0.22535711\n",
      "Iteration 74, loss = 0.21708509\n",
      "Iteration 75, loss = 0.20941325\n",
      "Iteration 76, loss = 0.20240222\n",
      "Iteration 77, loss = 0.19507580\n",
      "Iteration 78, loss = 0.18817211\n",
      "Iteration 79, loss = 0.18097383\n",
      "Iteration 80, loss = 0.17374008\n",
      "Iteration 81, loss = 0.16773546\n",
      "Iteration 82, loss = 0.16162644\n",
      "Iteration 83, loss = 0.15674361\n",
      "Iteration 84, loss = 0.15078868\n",
      "Iteration 85, loss = 0.14480545\n",
      "Iteration 86, loss = 0.13949857\n",
      "Iteration 87, loss = 0.13492772\n",
      "Iteration 88, loss = 0.13010554\n",
      "Iteration 89, loss = 0.12602207\n",
      "Iteration 90, loss = 0.12131053\n",
      "Iteration 91, loss = 0.11706832\n",
      "Iteration 92, loss = 0.11316722\n",
      "Iteration 93, loss = 0.10989357\n",
      "Iteration 94, loss = 0.10674732\n",
      "Iteration 95, loss = 0.10257472\n",
      "Iteration 96, loss = 0.09883882\n",
      "Iteration 97, loss = 0.09547391\n",
      "Iteration 98, loss = 0.09235753\n",
      "Iteration 99, loss = 0.08962487\n",
      "Iteration 100, loss = 0.08685505\n",
      "Iteration 101, loss = 0.08381940\n",
      "Iteration 102, loss = 0.08124676\n",
      "Iteration 103, loss = 0.07876701\n",
      "Iteration 104, loss = 0.07634491\n",
      "Iteration 105, loss = 0.07400955\n",
      "Iteration 106, loss = 0.07197757\n",
      "Iteration 107, loss = 0.06995247\n",
      "Iteration 108, loss = 0.06778704\n",
      "Iteration 109, loss = 0.06671250\n",
      "Iteration 110, loss = 0.06390048\n",
      "Iteration 111, loss = 0.06211880\n",
      "Iteration 112, loss = 0.06005577\n",
      "Iteration 113, loss = 0.05858064\n",
      "Iteration 114, loss = 0.05671224\n",
      "Iteration 115, loss = 0.05509387\n",
      "Iteration 116, loss = 0.05366178\n",
      "Iteration 117, loss = 0.05201353\n",
      "Iteration 118, loss = 0.05092129\n",
      "Iteration 119, loss = 0.04950266\n",
      "Iteration 120, loss = 0.04816344\n",
      "Iteration 121, loss = 0.04684056\n",
      "Iteration 122, loss = 0.04573800\n",
      "Iteration 123, loss = 0.04473956\n",
      "Iteration 124, loss = 0.04402093\n",
      "Iteration 125, loss = 0.04295271\n",
      "Iteration 126, loss = 0.04140691\n",
      "Iteration 127, loss = 0.04054420\n",
      "Iteration 128, loss = 0.03945047\n",
      "Iteration 129, loss = 0.03857945\n",
      "Iteration 130, loss = 0.03773544\n",
      "Iteration 131, loss = 0.03686884\n",
      "Iteration 132, loss = 0.03608847\n",
      "Iteration 133, loss = 0.03526178\n",
      "Iteration 134, loss = 0.03445007\n",
      "Iteration 135, loss = 0.03372594\n",
      "Iteration 136, loss = 0.03303335\n",
      "Iteration 137, loss = 0.03234819\n",
      "Iteration 138, loss = 0.03172576\n",
      "Iteration 139, loss = 0.03103893\n",
      "Iteration 140, loss = 0.03045708\n",
      "Iteration 141, loss = 0.02981660\n",
      "Iteration 142, loss = 0.02927216\n",
      "Iteration 143, loss = 0.02861910\n",
      "Iteration 144, loss = 0.02815070\n",
      "Iteration 145, loss = 0.02759849\n",
      "Iteration 146, loss = 0.02703675\n",
      "Iteration 147, loss = 0.02669390\n",
      "Iteration 148, loss = 0.02617310\n",
      "Iteration 149, loss = 0.02556642\n",
      "Iteration 150, loss = 0.02504658\n",
      "Iteration 151, loss = 0.02460292\n",
      "Iteration 152, loss = 0.02407863\n",
      "Iteration 153, loss = 0.02365449\n",
      "Iteration 154, loss = 0.02324643\n",
      "Iteration 155, loss = 0.02284540\n",
      "Iteration 156, loss = 0.02243324\n",
      "Iteration 157, loss = 0.02204404\n",
      "Iteration 158, loss = 0.02168108\n",
      "Iteration 159, loss = 0.02130515\n",
      "Iteration 160, loss = 0.02096060\n",
      "Iteration 161, loss = 0.02058820\n",
      "Iteration 162, loss = 0.02029350\n",
      "Iteration 163, loss = 0.02001784\n",
      "Iteration 164, loss = 0.01964320\n",
      "Iteration 165, loss = 0.01934551\n",
      "Iteration 166, loss = 0.01903760\n",
      "Iteration 167, loss = 0.01866391\n",
      "Iteration 168, loss = 0.01835398\n",
      "Iteration 169, loss = 0.01811285\n",
      "Iteration 170, loss = 0.01789647\n",
      "Iteration 171, loss = 0.01759042\n",
      "Iteration 172, loss = 0.01732643\n",
      "Iteration 173, loss = 0.01700357\n",
      "Iteration 174, loss = 0.01677289\n",
      "Iteration 175, loss = 0.01651977\n",
      "Iteration 176, loss = 0.01622301\n",
      "Iteration 177, loss = 0.01603133\n",
      "Iteration 178, loss = 0.01574594\n",
      "Iteration 179, loss = 0.01551710\n",
      "Iteration 180, loss = 0.01530676\n",
      "Iteration 181, loss = 0.01508508\n",
      "Iteration 182, loss = 0.01488022\n",
      "Iteration 183, loss = 0.01467054\n",
      "Iteration 184, loss = 0.01446502\n",
      "Iteration 185, loss = 0.01423823\n",
      "Iteration 186, loss = 0.01404931\n",
      "Iteration 187, loss = 0.01384505\n",
      "Iteration 188, loss = 0.01365841\n",
      "Iteration 189, loss = 0.01346038\n",
      "Iteration 190, loss = 0.01328433\n",
      "Iteration 191, loss = 0.01310816\n",
      "Iteration 192, loss = 0.01292527\n",
      "Iteration 193, loss = 0.01275019\n",
      "Iteration 194, loss = 0.01258652\n",
      "Iteration 195, loss = 0.01242594\n",
      "Iteration 196, loss = 0.01225466\n",
      "Iteration 197, loss = 0.01209350\n",
      "Iteration 198, loss = 0.01197755\n",
      "Iteration 199, loss = 0.01182267\n",
      "Iteration 200, loss = 0.01162436\n",
      "Iteration 201, loss = 0.01149954\n",
      "Iteration 202, loss = 0.01134095\n",
      "Iteration 203, loss = 0.01118199\n",
      "Iteration 204, loss = 0.01104422\n",
      "Iteration 205, loss = 0.01092409\n",
      "Iteration 206, loss = 0.01077867\n",
      "Iteration 207, loss = 0.01063382\n",
      "Iteration 208, loss = 0.01051310\n",
      "Iteration 209, loss = 0.01038164\n",
      "Iteration 210, loss = 0.01025440\n",
      "Iteration 211, loss = 0.01012832\n",
      "Iteration 212, loss = 0.01000944\n",
      "Iteration 213, loss = 0.00989890\n",
      "Iteration 214, loss = 0.00978444\n",
      "Iteration 215, loss = 0.00967767\n",
      "Iteration 216, loss = 0.00956768\n",
      "Iteration 217, loss = 0.00946056\n",
      "Iteration 218, loss = 0.00936445\n",
      "Iteration 219, loss = 0.00926067\n",
      "Iteration 220, loss = 0.00915894\n",
      "Iteration 221, loss = 0.00906940\n",
      "Iteration 222, loss = 0.00896579\n",
      "Iteration 223, loss = 0.00885725\n",
      "Iteration 224, loss = 0.00876204\n",
      "Iteration 225, loss = 0.00867428\n",
      "Iteration 226, loss = 0.00859193\n",
      "Iteration 227, loss = 0.00849515\n",
      "Iteration 228, loss = 0.00840815\n",
      "Iteration 229, loss = 0.00834268\n",
      "Iteration 230, loss = 0.00823927\n",
      "Iteration 231, loss = 0.00814791\n",
      "Iteration 232, loss = 0.00806211\n",
      "Iteration 233, loss = 0.00798000\n",
      "Iteration 234, loss = 0.00789160\n",
      "Iteration 235, loss = 0.00781139\n",
      "Iteration 236, loss = 0.00773807\n",
      "Iteration 237, loss = 0.00765775\n",
      "Iteration 238, loss = 0.00758477\n",
      "Iteration 239, loss = 0.00750719\n",
      "Iteration 240, loss = 0.00743464\n",
      "Iteration 241, loss = 0.00735828\n",
      "Iteration 242, loss = 0.00728458\n",
      "Iteration 243, loss = 0.00722632\n",
      "Iteration 244, loss = 0.00715654\n",
      "Iteration 245, loss = 0.00709820\n",
      "Iteration 246, loss = 0.00701577\n",
      "Iteration 247, loss = 0.00695941\n",
      "Iteration 248, loss = 0.00688376\n",
      "Iteration 249, loss = 0.00682225\n",
      "Iteration 250, loss = 0.00675021\n",
      "Iteration 251, loss = 0.00668795\n",
      "Iteration 252, loss = 0.00662971\n",
      "Iteration 253, loss = 0.00656670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.00650517\n",
      "Iteration 255, loss = 0.00644694\n",
      "Iteration 256, loss = 0.00639027\n",
      "Iteration 257, loss = 0.00633284\n",
      "Iteration 258, loss = 0.00627417\n",
      "Iteration 259, loss = 0.00621589\n",
      "Iteration 260, loss = 0.00616139\n",
      "Iteration 261, loss = 0.00611075\n",
      "Iteration 262, loss = 0.00605713\n",
      "Iteration 263, loss = 0.00600225\n",
      "Iteration 264, loss = 0.00595132\n",
      "Iteration 265, loss = 0.00590182\n",
      "Iteration 266, loss = 0.00585325\n",
      "Iteration 267, loss = 0.00580433\n",
      "Iteration 268, loss = 0.00575372\n",
      "Iteration 269, loss = 0.00570206\n",
      "Iteration 270, loss = 0.00565538\n",
      "Iteration 271, loss = 0.00560996\n",
      "Iteration 272, loss = 0.00556172\n",
      "Iteration 273, loss = 0.00551928\n",
      "Iteration 274, loss = 0.00547939\n",
      "Iteration 275, loss = 0.00542988\n",
      "Iteration 276, loss = 0.00538176\n",
      "Iteration 277, loss = 0.00533505\n",
      "Iteration 278, loss = 0.00529113\n",
      "Iteration 279, loss = 0.00525356\n",
      "Iteration 280, loss = 0.00521021\n",
      "Iteration 281, loss = 0.00516522\n",
      "Iteration 282, loss = 0.00512275\n",
      "Iteration 283, loss = 0.00509224\n",
      "Iteration 284, loss = 0.00504243\n",
      "Iteration 285, loss = 0.00500067\n",
      "Iteration 286, loss = 0.00496348\n",
      "Iteration 287, loss = 0.00492533\n",
      "Iteration 288, loss = 0.00488699\n",
      "Iteration 289, loss = 0.00484738\n",
      "Iteration 290, loss = 0.00480650\n",
      "Iteration 291, loss = 0.00477004\n",
      "Iteration 292, loss = 0.00473204\n",
      "Iteration 293, loss = 0.00469881\n",
      "Iteration 294, loss = 0.00465974\n",
      "Iteration 295, loss = 0.00462604\n",
      "Iteration 296, loss = 0.00459090\n",
      "Iteration 297, loss = 0.00455558\n",
      "Iteration 298, loss = 0.00452005\n",
      "Iteration 299, loss = 0.00449034\n",
      "Iteration 300, loss = 0.00446192\n",
      "Iteration 301, loss = 0.00442236\n",
      "Iteration 302, loss = 0.00439267\n",
      "Iteration 303, loss = 0.00435778\n",
      "Iteration 304, loss = 0.00432551\n",
      "Iteration 305, loss = 0.00429310\n",
      "Iteration 306, loss = 0.00425818\n",
      "Iteration 307, loss = 0.00422831\n",
      "Iteration 308, loss = 0.00419903\n",
      "Iteration 309, loss = 0.00416999\n",
      "Iteration 310, loss = 0.00413900\n",
      "Iteration 311, loss = 0.00410756\n",
      "Iteration 312, loss = 0.00407765\n",
      "Iteration 313, loss = 0.00404974\n",
      "Iteration 314, loss = 0.00402114\n",
      "Iteration 315, loss = 0.00399404\n",
      "Iteration 316, loss = 0.00396523\n",
      "Iteration 317, loss = 0.00394067\n",
      "Iteration 318, loss = 0.00391277\n",
      "Iteration 319, loss = 0.00388319\n",
      "Iteration 320, loss = 0.00385530\n",
      "Iteration 321, loss = 0.00382948\n",
      "Iteration 322, loss = 0.00380216\n",
      "Iteration 323, loss = 0.00377649\n",
      "Iteration 324, loss = 0.00375085\n",
      "Iteration 325, loss = 0.00372537\n",
      "Iteration 326, loss = 0.00370219\n",
      "Iteration 327, loss = 0.00367633\n",
      "Iteration 328, loss = 0.00365192\n",
      "Iteration 329, loss = 0.00362664\n",
      "Iteration 330, loss = 0.00360272\n",
      "Iteration 331, loss = 0.00357731\n",
      "Iteration 332, loss = 0.00355459\n",
      "Iteration 333, loss = 0.00353044\n",
      "Iteration 334, loss = 0.00350626\n",
      "Iteration 335, loss = 0.00348394\n",
      "Iteration 336, loss = 0.00346262\n",
      "Iteration 337, loss = 0.00343827\n",
      "Iteration 338, loss = 0.00341772\n",
      "Iteration 339, loss = 0.00339441\n",
      "Iteration 340, loss = 0.00337436\n",
      "Iteration 341, loss = 0.00335145\n",
      "Iteration 342, loss = 0.00332899\n",
      "Iteration 343, loss = 0.00330884\n",
      "Iteration 344, loss = 0.00328711\n",
      "Iteration 345, loss = 0.00326693\n",
      "Iteration 346, loss = 0.00324573\n",
      "Iteration 347, loss = 0.00322519\n",
      "Iteration 348, loss = 0.00320504\n",
      "Iteration 349, loss = 0.00318456\n",
      "Iteration 350, loss = 0.00316540\n",
      "Iteration 351, loss = 0.00314558\n",
      "Iteration 352, loss = 0.00312623\n",
      "Iteration 353, loss = 0.00310748\n",
      "Iteration 354, loss = 0.00308813\n",
      "Iteration 355, loss = 0.00306873\n",
      "Iteration 356, loss = 0.00304974\n",
      "Iteration 357, loss = 0.00302998\n",
      "Iteration 358, loss = 0.00301426\n",
      "Iteration 359, loss = 0.00299574\n",
      "Iteration 360, loss = 0.00297676\n",
      "Iteration 361, loss = 0.00295753\n",
      "Iteration 362, loss = 0.00293926\n",
      "Iteration 363, loss = 0.00292148\n",
      "Iteration 364, loss = 0.00290273\n",
      "Iteration 365, loss = 0.00288536\n",
      "Iteration 366, loss = 0.00286834\n",
      "Iteration 367, loss = 0.00285179\n",
      "Iteration 368, loss = 0.00283436\n",
      "Iteration 369, loss = 0.00281729\n",
      "Iteration 370, loss = 0.00280196\n",
      "Iteration 371, loss = 0.00278472\n",
      "Iteration 372, loss = 0.00276826\n",
      "Iteration 373, loss = 0.00275268\n",
      "Iteration 374, loss = 0.00273642\n",
      "Iteration 375, loss = 0.00272050\n",
      "Iteration 376, loss = 0.00270464\n",
      "Iteration 377, loss = 0.00269008\n",
      "Iteration 378, loss = 0.00267452\n",
      "Iteration 379, loss = 0.00265950\n",
      "Iteration 380, loss = 0.00264444\n",
      "Iteration 381, loss = 0.00262986\n",
      "Iteration 382, loss = 0.00261496\n",
      "Iteration 383, loss = 0.00259977\n",
      "Iteration 384, loss = 0.00258631\n",
      "Iteration 385, loss = 0.00257284\n",
      "Iteration 386, loss = 0.00255663\n",
      "Iteration 387, loss = 0.00254173\n",
      "Iteration 388, loss = 0.00252862\n",
      "Iteration 389, loss = 0.00251444\n",
      "Iteration 390, loss = 0.00249957\n",
      "Iteration 391, loss = 0.00248581\n",
      "Iteration 392, loss = 0.00247102\n",
      "Iteration 393, loss = 0.00245888\n",
      "Iteration 394, loss = 0.00244592\n",
      "Iteration 395, loss = 0.00243274\n",
      "Iteration 396, loss = 0.00241834\n",
      "Iteration 397, loss = 0.00240618\n",
      "Iteration 398, loss = 0.00239257\n",
      "Iteration 399, loss = 0.00237847\n",
      "Iteration 400, loss = 0.00236616\n",
      "Iteration 401, loss = 0.00235338\n",
      "Iteration 402, loss = 0.00234086\n",
      "Iteration 403, loss = 0.00232798\n",
      "Iteration 404, loss = 0.00231585\n",
      "Iteration 405, loss = 0.00230344\n",
      "Iteration 406, loss = 0.00229097\n",
      "Iteration 407, loss = 0.00227876\n",
      "Iteration 408, loss = 0.00226692\n",
      "Iteration 409, loss = 0.00225534\n",
      "Iteration 410, loss = 0.00224306\n",
      "Iteration 411, loss = 0.00223170\n",
      "Iteration 412, loss = 0.00221988\n",
      "Iteration 413, loss = 0.00220853\n",
      "Iteration 414, loss = 0.00219724\n",
      "Iteration 415, loss = 0.00218571\n",
      "Iteration 416, loss = 0.00217434\n",
      "Iteration 417, loss = 0.00216301\n",
      "Iteration 418, loss = 0.00215258\n",
      "Iteration 419, loss = 0.00214166\n",
      "Iteration 420, loss = 0.00213111\n",
      "Iteration 421, loss = 0.00211935\n",
      "Iteration 422, loss = 0.00210912\n",
      "Iteration 423, loss = 0.00209848\n",
      "Iteration 424, loss = 0.00208814\n",
      "Iteration 425, loss = 0.00207751\n",
      "Iteration 426, loss = 0.00206728\n",
      "Iteration 427, loss = 0.00205733\n",
      "Iteration 428, loss = 0.00204728\n",
      "Iteration 429, loss = 0.00203748\n",
      "Iteration 430, loss = 0.00202687\n",
      "Iteration 431, loss = 0.00201665\n",
      "Iteration 432, loss = 0.00200653\n",
      "Iteration 433, loss = 0.00199720\n",
      "Iteration 434, loss = 0.00198676\n",
      "Iteration 435, loss = 0.00197718\n",
      "Iteration 436, loss = 0.00196806\n",
      "Iteration 437, loss = 0.00195928\n",
      "Iteration 438, loss = 0.00194944\n",
      "Iteration 439, loss = 0.00193962\n",
      "Iteration 440, loss = 0.00193078\n",
      "Iteration 441, loss = 0.00192136\n",
      "Iteration 442, loss = 0.00191187\n",
      "Iteration 443, loss = 0.00190262\n",
      "Iteration 444, loss = 0.00189339\n",
      "Iteration 445, loss = 0.00188474\n",
      "Iteration 446, loss = 0.00187618\n",
      "Iteration 447, loss = 0.00186676\n",
      "Iteration 448, loss = 0.00185777\n",
      "Iteration 449, loss = 0.00184976\n",
      "Iteration 450, loss = 0.00184111\n",
      "Iteration 451, loss = 0.00183196\n",
      "Iteration 452, loss = 0.00182318\n",
      "Iteration 453, loss = 0.00181537\n",
      "Iteration 454, loss = 0.00180623\n",
      "Iteration 455, loss = 0.00179781\n",
      "Iteration 456, loss = 0.00179003\n",
      "Iteration 457, loss = 0.00178165\n",
      "Iteration 458, loss = 0.00177348\n",
      "Iteration 459, loss = 0.00176541\n",
      "Iteration 460, loss = 0.00175764\n",
      "Iteration 461, loss = 0.00174943\n",
      "Iteration 462, loss = 0.00174146\n",
      "Iteration 463, loss = 0.00173358\n",
      "Iteration 464, loss = 0.00172596\n",
      "Iteration 465, loss = 0.00171813\n",
      "Iteration 466, loss = 0.00171105\n",
      "Iteration 467, loss = 0.00170304\n",
      "Iteration 468, loss = 0.00169576\n",
      "Iteration 469, loss = 0.00168890\n",
      "Iteration 470, loss = 0.00168147\n",
      "Iteration 471, loss = 0.00167351\n",
      "Iteration 472, loss = 0.00166574\n",
      "Iteration 473, loss = 0.00165844\n",
      "Iteration 474, loss = 0.00165102\n",
      "Iteration 475, loss = 0.00164407\n",
      "Iteration 476, loss = 0.00163675\n",
      "Iteration 477, loss = 0.00162949\n",
      "Iteration 478, loss = 0.00162269\n",
      "Iteration 479, loss = 0.00161620\n",
      "Iteration 480, loss = 0.00160945\n",
      "Iteration 481, loss = 0.00160227\n",
      "Iteration 482, loss = 0.00159511\n",
      "Iteration 483, loss = 0.00158825\n",
      "Iteration 484, loss = 0.00158159\n",
      "Iteration 485, loss = 0.00157497\n",
      "Iteration 486, loss = 0.00156839\n",
      "Iteration 487, loss = 0.00156161\n",
      "Iteration 488, loss = 0.00155499\n",
      "Iteration 489, loss = 0.00154833\n",
      "Iteration 490, loss = 0.00154215\n",
      "Iteration 491, loss = 0.00153539\n",
      "Iteration 492, loss = 0.00152880\n",
      "Iteration 493, loss = 0.00152247\n",
      "Iteration 494, loss = 0.00151646\n",
      "Iteration 495, loss = 0.00151014\n",
      "Iteration 496, loss = 0.00150377\n",
      "Iteration 497, loss = 0.00149790\n",
      "Iteration 498, loss = 0.00149191\n",
      "Iteration 499, loss = 0.00148617\n",
      "Iteration 500, loss = 0.00147985\n",
      "Iteration 501, loss = 0.00147375\n",
      "Iteration 502, loss = 0.00146743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 503, loss = 0.00146141\n",
      "Iteration 504, loss = 0.00145566\n",
      "Iteration 505, loss = 0.00144980\n",
      "Iteration 506, loss = 0.00144399\n",
      "Iteration 507, loss = 0.00143845\n",
      "Iteration 508, loss = 0.00143280\n",
      "Iteration 509, loss = 0.00142700\n",
      "Iteration 510, loss = 0.00142114\n",
      "Iteration 511, loss = 0.00141552\n",
      "Iteration 512, loss = 0.00141004\n",
      "Iteration 513, loss = 0.00140496\n",
      "Iteration 514, loss = 0.00139921\n",
      "Iteration 515, loss = 0.00139346\n",
      "Iteration 516, loss = 0.00138771\n",
      "Iteration 517, loss = 0.00138242\n",
      "Iteration 518, loss = 0.00137738\n",
      "Iteration 519, loss = 0.00137218\n",
      "Iteration 520, loss = 0.00136631\n",
      "Iteration 521, loss = 0.00136186\n",
      "Iteration 522, loss = 0.00135633\n",
      "Iteration 523, loss = 0.00135099\n",
      "Iteration 524, loss = 0.00134552\n",
      "Iteration 525, loss = 0.00134052\n",
      "Iteration 526, loss = 0.00133539\n",
      "Iteration 527, loss = 0.00133005\n",
      "Iteration 528, loss = 0.00132505\n",
      "Iteration 529, loss = 0.00132011\n",
      "Iteration 530, loss = 0.00131524\n",
      "Iteration 531, loss = 0.00131015\n",
      "Iteration 532, loss = 0.00130543\n",
      "Iteration 533, loss = 0.00130069\n",
      "Iteration 534, loss = 0.00129557\n",
      "Iteration 535, loss = 0.00129095\n",
      "Iteration 536, loss = 0.00128593\n",
      "Iteration 537, loss = 0.00128119\n",
      "Iteration 538, loss = 0.00127657\n",
      "Iteration 539, loss = 0.00127174\n",
      "Iteration 540, loss = 0.00126714\n",
      "Iteration 541, loss = 0.00126250\n",
      "Iteration 542, loss = 0.00125798\n",
      "Iteration 543, loss = 0.00125310\n",
      "Iteration 544, loss = 0.00124843\n",
      "Iteration 545, loss = 0.00124387\n",
      "Iteration 546, loss = 0.00123938\n",
      "Iteration 547, loss = 0.00123491\n",
      "Iteration 548, loss = 0.00123048\n",
      "Iteration 549, loss = 0.00122601\n",
      "Iteration 550, loss = 0.00122199\n",
      "Iteration 551, loss = 0.00121777\n",
      "Iteration 552, loss = 0.00121347\n",
      "Iteration 553, loss = 0.00120925\n",
      "Iteration 554, loss = 0.00120484\n",
      "Iteration 555, loss = 0.00120008\n",
      "Iteration 556, loss = 0.00119634\n",
      "Iteration 557, loss = 0.00119164\n",
      "Iteration 558, loss = 0.00118773\n",
      "Iteration 559, loss = 0.00118343\n",
      "Iteration 560, loss = 0.00117918\n",
      "Iteration 561, loss = 0.00117512\n",
      "Iteration 562, loss = 0.00117094\n",
      "Iteration 563, loss = 0.00116678\n",
      "Iteration 564, loss = 0.00116272\n",
      "Iteration 565, loss = 0.00115881\n",
      "Iteration 566, loss = 0.00115470\n",
      "Iteration 567, loss = 0.00115091\n",
      "Iteration 568, loss = 0.00114693\n",
      "Iteration 569, loss = 0.00114308\n",
      "Iteration 570, loss = 0.00113943\n",
      "Iteration 571, loss = 0.00113580\n",
      "Iteration 572, loss = 0.00113154\n",
      "Iteration 573, loss = 0.00112765\n",
      "Iteration 574, loss = 0.00112393\n",
      "Iteration 575, loss = 0.00112016\n",
      "Iteration 576, loss = 0.00111638\n",
      "Iteration 577, loss = 0.00111278\n",
      "Iteration 578, loss = 0.00110892\n",
      "Iteration 579, loss = 0.00110532\n",
      "Iteration 580, loss = 0.00110159\n",
      "Iteration 581, loss = 0.00109808\n",
      "Iteration 582, loss = 0.00109456\n",
      "Iteration 583, loss = 0.00109049\n",
      "Iteration 584, loss = 0.00108739\n",
      "Iteration 585, loss = 0.00108377\n",
      "Iteration 586, loss = 0.00108008\n",
      "Iteration 587, loss = 0.00107661\n",
      "Iteration 588, loss = 0.00107310\n",
      "Iteration 589, loss = 0.00106954\n",
      "Iteration 590, loss = 0.00106603\n",
      "Iteration 591, loss = 0.00106277\n",
      "Iteration 592, loss = 0.00105949\n",
      "Iteration 593, loss = 0.00105653\n",
      "Iteration 594, loss = 0.00105279\n",
      "Iteration 595, loss = 0.00104900\n",
      "Iteration 596, loss = 0.00104565\n",
      "Iteration 597, loss = 0.00104240\n",
      "Iteration 598, loss = 0.00103931\n",
      "Iteration 599, loss = 0.00103583\n",
      "Iteration 600, loss = 0.00103259\n",
      "Iteration 601, loss = 0.00102917\n",
      "Iteration 602, loss = 0.00102615\n",
      "Iteration 603, loss = 0.00102279\n",
      "Iteration 604, loss = 0.00101959\n",
      "Iteration 605, loss = 0.00101630\n",
      "Iteration 606, loss = 0.00101316\n",
      "Iteration 607, loss = 0.00100996\n",
      "Iteration 608, loss = 0.00100691\n",
      "Iteration 609, loss = 0.00100390\n",
      "Iteration 610, loss = 0.00100080\n",
      "Iteration 611, loss = 0.00099772\n",
      "Iteration 612, loss = 0.00099458\n",
      "Iteration 613, loss = 0.00099161\n",
      "Iteration 614, loss = 0.00098862\n",
      "Iteration 615, loss = 0.00098569\n",
      "Iteration 616, loss = 0.00098260\n",
      "Iteration 617, loss = 0.00097991\n",
      "Iteration 618, loss = 0.00097703\n",
      "Iteration 619, loss = 0.00097391\n",
      "Iteration 620, loss = 0.00097103\n",
      "Iteration 621, loss = 0.00096821\n",
      "Iteration 622, loss = 0.00096525\n",
      "Iteration 623, loss = 0.00096248\n",
      "Iteration 624, loss = 0.00095968\n",
      "Iteration 625, loss = 0.00095684\n",
      "Iteration 626, loss = 0.00095385\n",
      "Iteration 627, loss = 0.00095101\n",
      "Iteration 628, loss = 0.00094819\n",
      "Iteration 629, loss = 0.00094560\n",
      "Iteration 630, loss = 0.00094273\n",
      "Iteration 631, loss = 0.00094004\n",
      "Iteration 632, loss = 0.00093732\n",
      "Iteration 633, loss = 0.00093474\n",
      "Iteration 634, loss = 0.00093197\n",
      "Iteration 635, loss = 0.00092930\n",
      "Iteration 636, loss = 0.00092671\n",
      "Iteration 637, loss = 0.00092387\n",
      "Iteration 638, loss = 0.00092132\n",
      "Iteration 639, loss = 0.00091880\n",
      "Iteration 640, loss = 0.00091607\n",
      "Iteration 641, loss = 0.00091347\n",
      "Iteration 642, loss = 0.00091083\n",
      "Iteration 643, loss = 0.00090827\n",
      "Iteration 644, loss = 0.00090583\n",
      "Iteration 645, loss = 0.00090319\n",
      "Iteration 646, loss = 0.00090073\n",
      "Iteration 647, loss = 0.00089820\n",
      "Iteration 648, loss = 0.00089568\n",
      "Iteration 649, loss = 0.00089317\n",
      "Iteration 650, loss = 0.00089078\n",
      "Iteration 651, loss = 0.00088838\n",
      "Iteration 652, loss = 0.00088582\n",
      "Iteration 653, loss = 0.00088337\n",
      "Iteration 654, loss = 0.00088105\n",
      "Iteration 655, loss = 0.00087867\n",
      "Iteration 656, loss = 0.00087628\n",
      "Iteration 657, loss = 0.00087414\n",
      "Iteration 658, loss = 0.00087171\n",
      "Iteration 659, loss = 0.00086920\n",
      "Iteration 660, loss = 0.00086693\n",
      "Iteration 661, loss = 0.00086460\n",
      "Iteration 662, loss = 0.00086230\n",
      "Iteration 663, loss = 0.00086027\n",
      "Iteration 664, loss = 0.00085788\n",
      "Iteration 665, loss = 0.00085539\n",
      "Iteration 666, loss = 0.00085308\n",
      "Iteration 667, loss = 0.00085106\n",
      "Iteration 668, loss = 0.00084866\n",
      "Iteration 669, loss = 0.00084641\n",
      "Iteration 670, loss = 0.00084419\n",
      "Iteration 671, loss = 0.00084200\n",
      "Iteration 672, loss = 0.00083992\n",
      "Iteration 673, loss = 0.00083753\n",
      "Iteration 674, loss = 0.00083558\n",
      "Iteration 675, loss = 0.00083327\n",
      "Iteration 676, loss = 0.00083108\n",
      "Iteration 677, loss = 0.00082896\n",
      "Iteration 678, loss = 0.00082698\n",
      "Iteration 679, loss = 0.00082461\n",
      "Iteration 680, loss = 0.00082265\n",
      "Iteration 681, loss = 0.00082053\n",
      "Iteration 682, loss = 0.00081842\n",
      "Iteration 683, loss = 0.00081627\n",
      "Iteration 684, loss = 0.00081435\n",
      "Iteration 685, loss = 0.00081240\n",
      "Iteration 686, loss = 0.00081031\n",
      "Iteration 687, loss = 0.00080825\n",
      "Iteration 688, loss = 0.00080609\n",
      "Iteration 689, loss = 0.00080406\n",
      "Iteration 690, loss = 0.00080204\n",
      "Iteration 691, loss = 0.00080004\n",
      "Iteration 692, loss = 0.00079810\n",
      "Iteration 693, loss = 0.00079624\n",
      "Iteration 694, loss = 0.00079437\n",
      "Iteration 695, loss = 0.00079233\n",
      "Iteration 696, loss = 0.00079041\n",
      "Iteration 697, loss = 0.00078853\n",
      "Iteration 698, loss = 0.00078657\n",
      "Iteration 699, loss = 0.00078468\n",
      "Iteration 700, loss = 0.00078285\n",
      "Iteration 701, loss = 0.00078102\n",
      "Iteration 702, loss = 0.00077902\n",
      "Iteration 703, loss = 0.00077718\n",
      "Iteration 704, loss = 0.00077530\n",
      "Iteration 705, loss = 0.00077362\n",
      "Iteration 706, loss = 0.00077165\n",
      "Iteration 707, loss = 0.00076986\n",
      "Iteration 708, loss = 0.00076800\n",
      "Iteration 709, loss = 0.00076618\n",
      "Iteration 710, loss = 0.00076441\n",
      "Iteration 711, loss = 0.00076268\n",
      "Iteration 712, loss = 0.00076087\n",
      "Iteration 713, loss = 0.00075910\n",
      "Iteration 714, loss = 0.00075731\n",
      "Iteration 715, loss = 0.00075571\n",
      "Iteration 716, loss = 0.00075389\n",
      "Iteration 717, loss = 0.00075214\n",
      "Iteration 718, loss = 0.00075037\n",
      "Iteration 719, loss = 0.00074868\n",
      "Iteration 720, loss = 0.00074696\n",
      "Iteration 721, loss = 0.00074531\n",
      "Iteration 722, loss = 0.00074359\n",
      "Iteration 723, loss = 0.00074192\n",
      "Iteration 724, loss = 0.00074030\n",
      "Iteration 725, loss = 0.00073868\n",
      "Iteration 726, loss = 0.00073697\n",
      "Iteration 727, loss = 0.00073534\n",
      "Iteration 728, loss = 0.00073368\n",
      "Iteration 729, loss = 0.00073204\n",
      "Iteration 730, loss = 0.00073046\n",
      "Iteration 731, loss = 0.00072884\n",
      "Iteration 732, loss = 0.00072722\n",
      "Iteration 733, loss = 0.00072570\n",
      "Iteration 734, loss = 0.00072406\n",
      "Iteration 735, loss = 0.00072258\n",
      "Iteration 736, loss = 0.00072098\n",
      "Iteration 737, loss = 0.00071940\n",
      "Iteration 738, loss = 0.00071781\n",
      "Iteration 739, loss = 0.00071624\n",
      "Iteration 740, loss = 0.00071468\n",
      "Iteration 741, loss = 0.00071314\n",
      "Iteration 742, loss = 0.00071158\n",
      "Iteration 743, loss = 0.00071017\n",
      "Iteration 744, loss = 0.00070862\n",
      "Iteration 745, loss = 0.00070713\n",
      "Iteration 746, loss = 0.00070565\n",
      "Iteration 747, loss = 0.00070424\n",
      "Iteration 748, loss = 0.00070261\n",
      "Iteration 749, loss = 0.00070112\n",
      "Iteration 750, loss = 0.00069967\n",
      "Iteration 751, loss = 0.00069823\n",
      "Iteration 752, loss = 0.00069671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 753, loss = 0.00069529\n",
      "Iteration 754, loss = 0.00069396\n",
      "Iteration 755, loss = 0.00069238\n",
      "Iteration 756, loss = 0.00069101\n",
      "Iteration 757, loss = 0.00068964\n",
      "Iteration 758, loss = 0.00068815\n",
      "Iteration 759, loss = 0.00068685\n",
      "Iteration 760, loss = 0.00068546\n",
      "Iteration 761, loss = 0.00068414\n",
      "Iteration 762, loss = 0.00068265\n",
      "Iteration 763, loss = 0.00068127\n",
      "Iteration 764, loss = 0.00067985\n",
      "Iteration 765, loss = 0.00067858\n",
      "Iteration 766, loss = 0.00067720\n",
      "Iteration 767, loss = 0.00067577\n",
      "Iteration 768, loss = 0.00067444\n",
      "Iteration 769, loss = 0.00067307\n",
      "Iteration 770, loss = 0.00067180\n",
      "Iteration 771, loss = 0.00067040\n",
      "Iteration 772, loss = 0.00066907\n",
      "Iteration 773, loss = 0.00066773\n",
      "Iteration 774, loss = 0.00066644\n",
      "Iteration 775, loss = 0.00066518\n",
      "Iteration 776, loss = 0.00066385\n",
      "Iteration 777, loss = 0.00066256\n",
      "Iteration 778, loss = 0.00066133\n",
      "Iteration 779, loss = 0.00066006\n",
      "Iteration 780, loss = 0.00065870\n",
      "Iteration 781, loss = 0.00065748\n",
      "Iteration 782, loss = 0.00065622\n",
      "Iteration 783, loss = 0.00065494\n",
      "Iteration 784, loss = 0.00065372\n",
      "Iteration 785, loss = 0.00065243\n",
      "Iteration 786, loss = 0.00065122\n",
      "Iteration 787, loss = 0.00065000\n",
      "Iteration 788, loss = 0.00064874\n",
      "Iteration 789, loss = 0.00064755\n",
      "Iteration 790, loss = 0.00064639\n",
      "Iteration 791, loss = 0.00064523\n",
      "Iteration 792, loss = 0.00064400\n",
      "Iteration 793, loss = 0.00064283\n",
      "Iteration 794, loss = 0.00064159\n",
      "Iteration 795, loss = 0.00064041\n",
      "Iteration 796, loss = 0.00063924\n",
      "Iteration 797, loss = 0.00063811\n",
      "Iteration 798, loss = 0.00063688\n",
      "Iteration 799, loss = 0.00063567\n",
      "Iteration 800, loss = 0.00063465\n",
      "Iteration 801, loss = 0.00063345\n",
      "Iteration 802, loss = 0.00063229\n",
      "Iteration 803, loss = 0.00063111\n",
      "Iteration 804, loss = 0.00063000\n",
      "Iteration 805, loss = 0.00062885\n",
      "Iteration 806, loss = 0.00062776\n",
      "Iteration 807, loss = 0.00062660\n",
      "Iteration 808, loss = 0.00062548\n",
      "Iteration 809, loss = 0.00062437\n",
      "Iteration 810, loss = 0.00062327\n",
      "Iteration 811, loss = 0.00062219\n",
      "Iteration 812, loss = 0.00062108\n",
      "Iteration 813, loss = 0.00062001\n",
      "Iteration 814, loss = 0.00061896\n",
      "Iteration 815, loss = 0.00061785\n",
      "Iteration 816, loss = 0.00061681\n",
      "Iteration 817, loss = 0.00061569\n",
      "Iteration 818, loss = 0.00061462\n",
      "Iteration 819, loss = 0.00061358\n",
      "Iteration 820, loss = 0.00061254\n",
      "Iteration 821, loss = 0.00061149\n",
      "Iteration 822, loss = 0.00061044\n",
      "Iteration 823, loss = 0.00060936\n",
      "Iteration 824, loss = 0.00060829\n",
      "Iteration 825, loss = 0.00060725\n",
      "Iteration 826, loss = 0.00060626\n",
      "Iteration 827, loss = 0.00060528\n",
      "Iteration 828, loss = 0.00060418\n",
      "Iteration 829, loss = 0.00060317\n",
      "Iteration 830, loss = 0.00060215\n",
      "Iteration 831, loss = 0.00060119\n",
      "Iteration 832, loss = 0.00060020\n",
      "Iteration 833, loss = 0.00059915\n",
      "Iteration 834, loss = 0.00059819\n",
      "Iteration 835, loss = 0.00059721\n",
      "Iteration 836, loss = 0.00059624\n",
      "Iteration 837, loss = 0.00059527\n",
      "Iteration 838, loss = 0.00059436\n",
      "Iteration 839, loss = 0.00059333\n",
      "Iteration 840, loss = 0.00059238\n",
      "Iteration 841, loss = 0.00059143\n",
      "Iteration 842, loss = 0.00059040\n",
      "Iteration 843, loss = 0.00058944\n",
      "Iteration 844, loss = 0.00058848\n",
      "Iteration 845, loss = 0.00058760\n",
      "Iteration 846, loss = 0.00058664\n",
      "Iteration 847, loss = 0.00058567\n",
      "Iteration 848, loss = 0.00058474\n",
      "Iteration 849, loss = 0.00058381\n",
      "Iteration 850, loss = 0.00058294\n",
      "Iteration 851, loss = 0.00058203\n",
      "Iteration 852, loss = 0.00058111\n",
      "Iteration 853, loss = 0.00058018\n",
      "Iteration 854, loss = 0.00057925\n",
      "Iteration 855, loss = 0.00057832\n",
      "Iteration 856, loss = 0.00057751\n",
      "Iteration 857, loss = 0.00057659\n",
      "Iteration 858, loss = 0.00057575\n",
      "Iteration 859, loss = 0.00057479\n",
      "Iteration 860, loss = 0.00057390\n",
      "Iteration 861, loss = 0.00057306\n",
      "Iteration 862, loss = 0.00057209\n",
      "Iteration 863, loss = 0.00057137\n",
      "Iteration 864, loss = 0.00057047\n",
      "Iteration 865, loss = 0.00056951\n",
      "Iteration 866, loss = 0.00056868\n",
      "Iteration 867, loss = 0.00056781\n",
      "Iteration 868, loss = 0.00056697\n",
      "Iteration 869, loss = 0.00056611\n",
      "Iteration 870, loss = 0.00056528\n",
      "Iteration 871, loss = 0.00056449\n",
      "Iteration 872, loss = 0.00056365\n",
      "Iteration 873, loss = 0.00056280\n",
      "Iteration 874, loss = 0.00056196\n",
      "Iteration 875, loss = 0.00056111\n",
      "Iteration 876, loss = 0.00056030\n",
      "Iteration 877, loss = 0.00055946\n",
      "Iteration 878, loss = 0.00055865\n",
      "Iteration 879, loss = 0.00055781\n",
      "Iteration 880, loss = 0.00055704\n",
      "Iteration 881, loss = 0.00055624\n",
      "Iteration 882, loss = 0.00055537\n",
      "Iteration 883, loss = 0.00055460\n",
      "Iteration 884, loss = 0.00055381\n",
      "Iteration 885, loss = 0.00055297\n",
      "Iteration 886, loss = 0.00055216\n",
      "Iteration 887, loss = 0.00055137\n",
      "Iteration 888, loss = 0.00055058\n",
      "Iteration 889, loss = 0.00054976\n",
      "Iteration 890, loss = 0.00054904\n",
      "Iteration 891, loss = 0.00054828\n",
      "Iteration 892, loss = 0.00054750\n",
      "Iteration 893, loss = 0.00054670\n",
      "Iteration 894, loss = 0.00054589\n",
      "Iteration 895, loss = 0.00054512\n",
      "Iteration 896, loss = 0.00054433\n",
      "Iteration 897, loss = 0.00054365\n",
      "Iteration 898, loss = 0.00054286\n",
      "Iteration 899, loss = 0.00054210\n",
      "Iteration 900, loss = 0.00054139\n",
      "Iteration 901, loss = 0.00054062\n",
      "Iteration 902, loss = 0.00053986\n",
      "Iteration 903, loss = 0.00053911\n",
      "Iteration 904, loss = 0.00053840\n",
      "Iteration 905, loss = 0.00053767\n",
      "Iteration 906, loss = 0.00053695\n",
      "Iteration 907, loss = 0.00053616\n",
      "Iteration 908, loss = 0.00053545\n",
      "Iteration 909, loss = 0.00053477\n",
      "Iteration 910, loss = 0.00053414\n",
      "Iteration 911, loss = 0.00053329\n",
      "Iteration 912, loss = 0.00053258\n",
      "Iteration 913, loss = 0.00053191\n",
      "Iteration 914, loss = 0.00053118\n",
      "Iteration 915, loss = 0.00053042\n",
      "Iteration 916, loss = 0.00052969\n",
      "Iteration 917, loss = 0.00052906\n",
      "Iteration 918, loss = 0.00052830\n",
      "Iteration 919, loss = 0.00052763\n",
      "Iteration 920, loss = 0.00052691\n",
      "Iteration 921, loss = 0.00052625\n",
      "Iteration 922, loss = 0.00052557\n",
      "Iteration 923, loss = 0.00052488\n",
      "Iteration 924, loss = 0.00052421\n",
      "Iteration 925, loss = 0.00052352\n",
      "Iteration 926, loss = 0.00052279\n",
      "Iteration 927, loss = 0.00052213\n",
      "Iteration 928, loss = 0.00052145\n",
      "Iteration 929, loss = 0.00052079\n",
      "Iteration 930, loss = 0.00052011\n",
      "Iteration 931, loss = 0.00051948\n",
      "Iteration 932, loss = 0.00051880\n",
      "Iteration 933, loss = 0.00051813\n",
      "Iteration 934, loss = 0.00051750\n",
      "Iteration 935, loss = 0.00051687\n",
      "Iteration 936, loss = 0.00051617\n",
      "Iteration 937, loss = 0.00051549\n",
      "Iteration 938, loss = 0.00051489\n",
      "Iteration 939, loss = 0.00051429\n",
      "Iteration 940, loss = 0.00051363\n",
      "Iteration 941, loss = 0.00051291\n",
      "Iteration 942, loss = 0.00051228\n",
      "Iteration 943, loss = 0.00051167\n",
      "Iteration 944, loss = 0.00051104\n",
      "Iteration 945, loss = 0.00051041\n",
      "Iteration 946, loss = 0.00050976\n",
      "Iteration 947, loss = 0.00050913\n",
      "Iteration 948, loss = 0.00050852\n",
      "Iteration 949, loss = 0.00050790\n",
      "Iteration 950, loss = 0.00050727\n",
      "Iteration 951, loss = 0.00050666\n",
      "Iteration 952, loss = 0.00050604\n",
      "Iteration 953, loss = 0.00050541\n",
      "Iteration 954, loss = 0.00050480\n",
      "Iteration 955, loss = 0.00050421\n",
      "Iteration 956, loss = 0.00050357\n",
      "Iteration 957, loss = 0.00050300\n",
      "Iteration 958, loss = 0.00050245\n",
      "Iteration 959, loss = 0.00050174\n",
      "Iteration 960, loss = 0.00050113\n",
      "Iteration 961, loss = 0.00050053\n",
      "Iteration 962, loss = 0.00049998\n",
      "Iteration 963, loss = 0.00049939\n",
      "Iteration 964, loss = 0.00049879\n",
      "Iteration 965, loss = 0.00049818\n",
      "Iteration 966, loss = 0.00049761\n",
      "Iteration 967, loss = 0.00049702\n",
      "Iteration 968, loss = 0.00049641\n",
      "Iteration 969, loss = 0.00049586\n",
      "Iteration 970, loss = 0.00049527\n",
      "Iteration 971, loss = 0.00049469\n",
      "Iteration 972, loss = 0.00049411\n",
      "Iteration 973, loss = 0.00049354\n",
      "Iteration 974, loss = 0.00049295\n",
      "Iteration 975, loss = 0.00049239\n",
      "Iteration 976, loss = 0.00049181\n",
      "Iteration 977, loss = 0.00049126\n",
      "Iteration 978, loss = 0.00049067\n",
      "Iteration 979, loss = 0.00049012\n",
      "Iteration 980, loss = 0.00048955\n",
      "Iteration 981, loss = 0.00048899\n",
      "Iteration 982, loss = 0.00048842\n",
      "Iteration 983, loss = 0.00048786\n",
      "Iteration 984, loss = 0.00048732\n",
      "Iteration 985, loss = 0.00048679\n",
      "Iteration 986, loss = 0.00048626\n",
      "Iteration 987, loss = 0.00048574\n",
      "Iteration 988, loss = 0.00048513\n",
      "Iteration 989, loss = 0.00048458\n",
      "Iteration 990, loss = 0.00048406\n",
      "Iteration 991, loss = 0.00048349\n",
      "Iteration 992, loss = 0.00048295\n",
      "Iteration 993, loss = 0.00048243\n",
      "Iteration 994, loss = 0.00048190\n",
      "Iteration 995, loss = 0.00048136\n",
      "Iteration 996, loss = 0.00048082\n",
      "Iteration 997, loss = 0.00048028\n",
      "Iteration 998, loss = 0.00047975\n",
      "Iteration 999, loss = 0.00047927\n",
      "Iteration 1000, loss = 0.00047876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dapodik\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = clf.loss_curve_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x122eb810>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAFe9JREFUeJzt3X+M3Hd95/Hne2fXsZ2kOI434POPOAiXtkQlgW2alJ6Uo9ALEUp0Kr1LdCqBA1lq4YAK3Yn0pFCQ7g+kO+C49AC35AgIhR8h4tycOUiBquXuCNmkjkli0pgW4j0C3iS2E+P88Nrv+2O+a4/HM/Od3cx69jN5PqTRzHy/n515f/drvfzZz3zm+4nMRJI0WsaGXYAkafAMd0kaQYa7JI0gw12SRpDhLkkjyHCXpBFkuEvSCDLcJWkEGe6SNILGh/XG69atyy1btgzr7SWpSPfee+/jmTlZ125o4b5lyxamp6eH9faSVKSI+Ek/7RyWkaQRZLhL0ggy3CVpBBnukjSCDHdJGkGGuySNIMNdkkZQceH+8M+e5qPffJjHDz837FIkadkqLtwf2f80n/j2Xp78xfPDLkWSlq3iwj0IAFzXW5K6Ky/cm9lOYrpLUjflhXt1b89dkrqrDfeIWBkR34+I+yPiwYj4UIc2b4uI2YjYVd3euTTltvTcDXdJ6qqfq0I+B7w+Mw9HxATw3Yj4emZ+r63dlzLz3YMvsV015u6wjCR1VRvumZnA4erpRHUbWrLO99wlSd31NeYeEY2I2AXsB+7KzLs7NPu9iNgdEbdHxKaBVtmBwzKS1F1f4Z6ZxzLzEmAjcFlEXNzW5C+BLZn568BfAbd2ep2I2BYR0xExPTs7u6iC7bhLUr0FzZbJzIPAXwNXtW1/IjPnvzL658Bru/z89sycysypycnaVaI6inCeuyTV6We2zGRErKkerwLeAPywrc36lqfXAHsGWeQp71Xd+4GqJHXXz2yZ9cCtEdGg+Z/BlzPzzoj4MDCdmTuA90TENcAc8CTwtqUq2KmQklSvn9kyu4FLO2y/qeXxjcCNgy2ts5PfUJUkdVPgN1Tnx9yNd0nqprhwx567JNUqLty9towk1Ssv3OPkfBlJUmflhXt1b89dkrorL9wdc5ekWuWFuxcgkKRaxYX7PIdlJKm74sL95DdUTXdJ6qa8cK/ujXZJ6q64cMdry0hSreLCPVxmT5JqlRfujstIUq3ywr26N9slqbvywt2VmCSpVoHh3rx3zF2Suisv3Kt7e+6S1F154e61ZSSpVnHhjisxSVKt2nCPiJUR8f2IuD8iHoyID3Voc1ZEfCki9kbE3RGxZSmKbb7XUr2yJI2OfnruzwGvz8xXA5cAV0XE5W1t3gEcyMxXAB8DPjLYMk9nv12SuqsN92w6XD2dqG7t2XotcGv1+HbgdyKWpo994kVNd0nqqq8x94hoRMQuYD9wV2be3dZkA7APIDPngEPA+R1eZ1tETEfE9Ozs7KIKPjHP3XSXpK76CvfMPJaZlwAbgcsi4uK2Jp166aelb2Zuz8ypzJyanJxceLU4FVKS+rGg2TKZeRD4a+Cqtl0zwCaAiBgHXgI8OYD6ThNeFVKSavUzW2YyItZUj1cBbwB+2NZsB3BD9fgtwLdzieYqnrwqpCSpm/E+2qwHbo2IBs3/DL6cmXdGxIeB6czcAXwG+HxE7KXZY79uqQp2JSZJqlcb7pm5G7i0w/abWh4/C/z+YEurqetMvpkkFaa4b6g65i5J9coLd6/oLkm1ygt3e+6SVKvccB9uGZK0rJUX7h2/LyVJalVcuM9zWEaSuisu3F1mT5LqlRfu1b09d0nqrrxw9wNVSapVXLi7zJ4k1Ssu3F1mT5LqlRfu1b0dd0nqrrxwdyUmSapVXrhX9/bcJam78sLda8tIUq3ywt2VmCSpVnnh7kpMklSruHCXJNXrZ4HsTRHxnYjYExEPRsR7O7S5MiIORcSu6nZTp9caJPvtktRdPwtkzwHvz8z7IuJc4N6IuCszH2pr97eZ+ebBl3iqcCEmSapV23PPzMcy877q8dPAHmDDUhfWjfPcJanegsbcI2ILcClwd4fdV0TE/RHx9Yh4VZef3xYR0xExPTs7u+BiwXnuktSPvsM9Is4Bvgq8LzOfatt9H3BhZr4a+K/A1zq9RmZuz8ypzJyanJxcVMFeFVKS6vUV7hExQTPYv5CZd7Tvz8ynMvNw9XgnMBER6wZa6XwtJ64KuRSvLkmjoZ/ZMgF8BtiTmR/t0uZlVTsi4rLqdZ8YZKEn36t575i7JHXXz2yZ1wF/APwgInZV2/4E2AyQmZ8C3gL8YUTMAc8A1+USfcvIMXdJqlcb7pn5XU5marc2NwM3D6qonhxzl6RaxX1DNfDKYZJUp7xwt+cuSbXKC/fq3o67JHVXXri7iKok1Sou3Od5yV9J6q64cPe6YZJUr7xwd7KMJNUqL9xdZk+SahUX7rjMniTVKi7cnSwjSfXKC/fq3o67JHVXXri7EpMk1Sov3Kt7e+6S1F154e61ZSSpVnnh7kpMklSrvHB3JSZJqlVcuEuS6hUX7l5+QJLq9bNA9qaI+E5E7ImIByPivR3aRER8IiL2RsTuiHjN0pQLjSrdjx833SWpm34WyJ4D3p+Z90XEucC9EXFXZj7U0uZNwNbq9pvAJ6v7gRubD3ezXZK6qu25Z+ZjmXlf9fhpYA+woa3ZtcDnsul7wJqIWD/wajk5LHPccRlJ6mpBY+4RsQW4FLi7bdcGYF/L8xlO/w+AiNgWEdMRMT07O7uwSk++BhFeOEySeuk73CPiHOCrwPsy86n23R1+5LT0zcztmTmVmVOTk5MLq7TFWITDMpLUQ1/hHhETNIP9C5l5R4cmM8CmlucbgZ++8PI6GwuHZSSpl35mywTwGWBPZn60S7MdwFurWTOXA4cy87EB1tlekz13Seqhn9kyrwP+APhBROyqtv0JsBkgMz8F7ASuBvYCR4C3D77Uk+y5S1JvteGemd+l85h6a5sE3jWoouo0IpznLkk9FPcNVfADVUmqU2S4h8MyktRTkeE+NhbOc5ekHsoMd4dlJKmnQsPdYRlJ6qXIcHeeuyT1VmS4j4WX/JWkXooM90aEwzKS1EOR4e6wjCT1VmS4j415yV9J6qXMcHdYRpJ6Kjjch12FJC1fRYa7lx+QpN6KDHeHZSSpt0LDHY4fH3YVkrR8FRru9twlqZeCw33YVUjS8lVmuDvPXZJ66meB7FsiYn9EPNBl/5URcSgidlW3mwZf5qkclpGk3vpZIPuzwM3A53q0+dvMfPNAKuqDlx+QpN5qe+6Z+TfAk2eglr55PXdJ6m1QY+5XRMT9EfH1iHjVgF6zK4dlJKm3foZl6twHXJiZhyPiauBrwNZODSNiG7ANYPPmzYt+w0aE89wlqYcX3HPPzKcy83D1eCcwERHrurTdnplTmTk1OTm56Pf08gOS1NsLDveIeFlERPX4suo1n3ihr9vLWARmuyR1VzssExG3AVcC6yJiBvggMAGQmZ8C3gL8YUTMAc8A1+UST0IfG4Ojx0x3SeqmNtwz8/qa/TfTnCp5xoxFcMyuuyR1VeQ3VBtj4QLZktRDkeE+PhYOy0hSD4WG+xjH7LlLUldFhnujEcw50V2Suioy3MfHgjl77pLUVZHh3hgL5hxzl6Suigz3CcfcJamnIsO9OeZuuEtSN0WGe3PM3Q9UJambQsN9jGOOuUtSV2WGu8MyktRTkeHecFhGknoqMtwnnOcuST0VGe6NsTEy8eJhktRFkeE+3ggAjjo0I0kdlRnuY81w94tMktRZkeHeqMLdcXdJ6qzIcJ/vuXt9GUnqrDbcI+KWiNgfEQ902R8R8YmI2BsRuyPiNYMv81TjjWbZToeUpM766bl/Friqx/43AVur2zbgky+8rN4cc5ek3mrDPTP/BniyR5Nrgc9l0/eANRGxflAFdtJwWEaSehrEmPsGYF/L85lq25KZnwrpB6qS1Nkgwj06bOuYuhGxLSKmI2J6dnZ20W84PtYs+5hj7pLU0SDCfQbY1PJ8I/DTTg0zc3tmTmXm1OTk5KLfcH7M/ajDMpLU0SDCfQfw1mrWzOXAocx8bACv21XDD1QlqafxugYRcRtwJbAuImaADwITAJn5KWAncDWwFzgCvH2pip03cWIqpOEuSZ3UhntmXl+zP4F3DayiPpzsuTvmLkmdFP0NVcfcJamzMsO9MT9bxnCXpE6KDHcvHCZJvRUZ7icvHOaYuyR1Uma4+w1VSeqpzHAfc8xdknopMtwbJ2bLOCwjSZ0UGe4TDb+hKkm9FBnuXvJXknorMtzPGm8A8NzcsSFXIknLU5HhvmpFM9yfOWq4S1InZYb7RDPcjzxvuEtSJ0WGe2MsWDE+Zs9dkrooMtyh2Xt/1p67JHVUbLivXtFwWEaSuig23FdNNByWkaQuig33lRMNnjXcJamjYsPdYRlJ6q6vcI+IqyLi4YjYGxEf6LD/bRExGxG7qts7B1/qqVatcFhGkrrpZ4HsBvBnwBuBGeCeiNiRmQ+1Nf1SZr57CWrsaOVEg9mnnztTbydJRemn534ZsDcz/yEznwe+CFy7tGXVW23PXZK66ifcNwD7Wp7PVNva/V5E7I6I2yNi00Cq62HVRINnHHOXpI76CffosK39cox/CWzJzF8H/gq4teMLRWyLiOmImJ6dnV1YpW1WOhVSkrrqJ9xngNae+Ebgp60NMvOJzJwfAP9z4LWdXigzt2fmVGZOTU5OLqbeE1avaPbcM73sryS16yfc7wG2RsRFEbECuA7Y0dogIta3PL0G2DO4Ejs7Z+U4c8fT3rskdVA7WyYz5yLi3cA3gAZwS2Y+GBEfBqYzcwfwnoi4BpgDngTetoQ1A7B29QoADhw5yuoVtYchSS8qfaViZu4EdrZtu6nl8Y3AjYMtrbc18+H+i+fZsGbVmXxrSVr2iv2G6tqz53vuzw+5EklafooN9/NWTwDNYRlJ0qmKDff5YZmD9twl6TQFh3vVc/+FPXdJaldsuE80xjh35bhj7pLUQbHhDnDBuWfx86eeHXYZkrTsFB3uG89bzb4DR4ZdhiQtO0WH+6a1q5g58Mywy5CkZafocN943moOHjnK08/6oaoktSo63DedtxqAfU/ae5ekVkWH+6+uPxeA3TMHh1yJJC0vRYf7RevO5vyzV3DPjw8MuxRJWlaKDveI4LUXnsf0T54cdimStKwUHe4Av7FlLT954giPHXLcXZLmFR/u/+xXLgBg5w9+NuRKJGn5KD7cX3HBOVy84Ze47fuPcuy4S+5JEoxAuAP80ZWvYO/+w3z1vplhlyJJy8JIhPubLn4Zl25ew3/8n3t49AkvRyBJfYV7RFwVEQ9HxN6I+ECH/WdFxJeq/XdHxJZBF1pTHx//V5dw/HjyL/7b/2b6x86ekfTiVhvuEdEA/gx4E/BrwPUR8Wttzd4BHMjMVwAfAz4y6ELrXHj+2dzxR7/FOSvH+Zef/r/8u6/cz//50eMcfm7uTJciSUPXzwLZlwF7M/MfACLii8C1wEMtba4F/rR6fDtwc0REZp7RTzi3vvRc7vy3v81//ubf88V7HuUr984QAVsvOIdLNq3hl196LmvPXsGa1ROsWb2Cl6yaYEVjjPFGMNEYY2JsjInxYHxsjIlGEBFnsnxJGph+wn0DsK/l+Qzwm93aZOZcRBwCzgceH0SRC3Huygn+9JpX8cdv/GX+7tED7Np3kF37DvLNh37Ol6cX9oHr+Fg0g39sDKqcn4/7+eCPlu0ntp1oM/9Kp7c99fnJ/e2vT3vbDj/TXlfb22qB/LUtnB2hhbnuNzbxzn/68iV9j37CvdNZa++R99OGiNgGbAPYvHlzH2+9eC9ZNcGVr7yAK1/ZnAefmRx65igHjxzlwJHnOXjkKE89e5Tn544zdzw5euw4R48lc8eOn3x8vHl/9Njx6jXaDrDakC37sjrsk89p+9m2/S0/1+1n5l+T0/bnae3b92lh/K0tgr+0BVt3zllL/h79hPsMsKnl+Ubgp13azETEOPAS4LRPNTNzO7AdYGpq6oz+k4gI1qxewZrVK9jC2WfyrSXpjOtntsw9wNaIuCgiVgDXATva2uwAbqgevwX49pkeb5cknVTbc6/G0N8NfANoALdk5oMR8WFgOjN3AJ8BPh8Re2n22K9byqIlSb31MyxDZu4EdrZtu6nl8bPA7w+2NEnSYo3EN1QlSacy3CVpBBnukjSCDHdJGkGGuySNoBjWdPSImAV+ssgfX8cQLm0wZB7zi4PH/OLwQo75wsycrGs0tHB/ISJiOjOnhl3HmeQxvzh4zC8OZ+KYHZaRpBFkuEvSCCo13LcPu4Ah8JhfHDzmF4clP+Yix9wlSb2V2nOXJPVQXLjXLdZdqojYFBHfiYg9EfFgRLy32r42Iu6KiEeq+/Oq7RERn6h+D7sj4jXDPYLFiYhGRPxdRNxZPb+oWmT9kWrR9RXV9qEuwj5IEbEmIm6PiB9W5/uKUT7PEfHH1b/pByLitohYOYrnOSJuiYj9EfFAy7YFn9eIuKFq/0hE3NDpvfpRVLj3uVh3qeaA92fmrwKXA++qju0DwLcycyvwreo5NH8HW6vbNuCTZ77kgXgvsKfl+UeAj1XHe4Dm4uuwDBZhH6D/AvyvzPwV4NU0j38kz3NEbADeA0xl5sU0Lxt+HaN5nj8LXNW2bUHnNSLWAh+kuZTpZcAH5/9DWLDMLOYGXAF8o+X5jcCNw65riY71fwBvBB4G1lfb1gMPV48/DVzf0v5Eu1JuNFf1+hbweuBOmss1Pg6Mt59vmusJXFE9Hq/axbCPYRHH/EvAP7bXPqrnmZPrK6+tztudwD8f1fMMbAEeWOx5Ba4HPt2y/ZR2C7kV1XOn82LdG4ZUy5Kp/hS9FLgbeGlmPgZQ3V9QNRuF38XHgX8PHK+enw8czMy56nnrMZ2yCDswvwh7aV4OzAL/vRqO+ouIOJsRPc+Z+f+A/wQ8CjxG87zdy+if53kLPa8DO9+lhXtfC3GXLCLOAb4KvC8zn+rVtMO2Yn4XEfFmYH9m3tu6uUPT7GNfScaB1wCfzMxLgV9w8k/1Too+7mpI4VrgIuCfAGfTHJJoN2rnuU634xzY8ZcW7v0s1l2siJigGexfyMw7qs0/j4j11f71wP5qe+m/i9cB10TEj4Ev0hya+TiwplpkHU49phPH22sR9gLMADOZeXf1/HaaYT+q5/kNwD9m5mxmHgXuAH6L0T/P8xZ6Xgd2vksL934W6y5SRATNtWj3ZOZHW3a1Lj5+A82x+Pntb60+db8cODT/518JMvPGzNyYmVtonsdvZ+a/Br5Dc5F1OP14i1+EPTN/BuyLiFdWm34HeIgRPc80h2Muj4jV1b/x+eMd6fPcYqHn9RvA70bEedVfPb9bbVu4YX8AsYgPLK4G/h74EfAfhl3PAI/rt2n++bUb2FXdrqY53vgt4JHqfm3VPmjOHPoR8AOasxGGfhyLPPYrgTurxy8Hvg/sBb4CnFVtX1k931vtf/mw634Bx3sJMF2d668B543yeQY+BPwQeAD4PHDWKJ5n4DaanyscpdkDf8dizivwb6rj3wu8fbH1+A1VSRpBpQ3LSJL6YLhL0ggy3CVpBBnukjSCDHdJGkGGuySNIMNdkkaQ4S5JI+j/A6OJGN8/WKdPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
